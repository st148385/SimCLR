

# Devices
num_cpu_threads = 10
set_devices.num_cpu_threads = %num_cpu_threads
set_devices.soft_device_placement = False

# Input Pipeline
ds_name = 'cifar10'    #'cifar10'  #'tf_flowers'       #Speichert in C:\Users\Mari\tensorflow_datasets
tfds_path = '~\\tensorflow_datasets'        # if linux ~/tensorflow_datasets on server /data/public/tensorflow_datasets

size_batch = 512
dataset_image_size = 32     #Nach Augmentation haben x_i und x_j Auflösung: (image_size_dataset x image_size_dataset x 3)

gen_pipeline_train.ds_name = %ds_name
gen_pipeline_train.tfds_path = %tfds_path
gen_pipeline_train.size_batch = %size_batch
gen_pipeline_train.b_shuffle = True
gen_pipeline_train.dataset_cache = False
gen_pipeline_train.size_buffer_cpu = 5
gen_pipeline_train.shuffle_buffer_size = 50000
gen_pipeline_train.num_parallel_calls = %num_cpu_threads
gen_pipeline_train.x_size = %dataset_image_size
plot_dataset.dataset_name = %ds_name

# Training parameters
train.n_epochs=2
train.learning_rate_noScheduling=0.001
train.lr_max_ifScheduling=0.001
train.save_period = 1
train.size_batch = %size_batch
train.tau = 0.1
train.use_split_model = True     #23.5.2020: Es klappen folgende Mögl. (mit summary-Anzeige): (False, True) || (False, False) || (True, False) || (True, True)
train.use_2optimizers = False
train.use_learning_rate_scheduling = True  #falsefalse,falsetrue,truefalse,truetrue
